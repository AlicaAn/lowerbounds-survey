\chapter{Determinantal Complexity lower bounds}\label{chap:dc}

Recall that any polynomial $f$ that is computable by an ABP of polynomial size can be written as a projection of the determinant (\autoref{thm:vp}). 
Thus, a direct way to prove lower bounds is to find an explicit polynomial that requires super-polynomially large determinants to compute it. 
Hence, for a polynomial $f$, it is natural to ask ``If $f$ is to be written as a projection of an $m\times m$ determinant, how large should $m$ be?''
This is called the \emph{determinantal complexity} of $f$. 

\begin{definition}
  Let $f$ be an $n$-variate polynomial.
  The determinantal complexity of $f(\vecx)$, denoted by $\dc(f)$ is the smallest $m$ such that there is an $m\times m$ matrix $A(\vecx)$ with each entry being an linear function in $\vecx$ such that $f = \det(A(\vecx))$. 
\end{definition}

\noindent
By \autoref{thm:vp}, if we could show that $\dc(\Perm_n) = n^{\omega(1)}$, then this immediately would imply super-polynomial circuit lower bounds. 

\section{Introduction}
\begin{definition}
  The determinantal complexity of a polynomial $f$, over $n$ variables, is the minimum $m$ such that there are affine linear functions $A_{k,\ell}$, $1\leq k,\ell\leq m$ defined over the same set of variables and $f= \det((A_{k,\ell})_{1 \leq k,\ell \leq m})$. It is denoted by $\dc(f)$.  
\end{definition}
To resolve Valiant's hypothesis, proving $\dc(\Perm_n) = n^{\omega(\log n)}$ is sufficient. Von zur Gathen \cite{von1986}  proved $\dc(\Perm_n) \geq \sqrt{\frac{8}{7}}n$. Later Cai\cite{cai1990}, Babai and Seress \cite{von1987}, and Meshulam\cite{mesh1989} independently improved the lower bound to $\sqrt{2}n$. In 2004, Mignon and Ressayre\cite{mr04} came up with a new idea of using second order derivatives and proved that $\dc(\Perm_n) \geq \frac{n^2}{2}$ over the fields of characteristic zero. Subsequently, Cai et al.\cite{ccl2008} extended the result of Mignon and Ressayre to all fields of characteristic $\neq 2$.

For any polynomial $f$, Valiant proved that $\dc(f) \leq 2(F(f)+ 1)$ where $F(f)$ is the arithmetic formula complexity of $f$ \cite{v79}. It can be seen that $\dc(f) = O(B(f))$ where $B(f)$ is the arithmetic branching program complexity of $f$ \cite{mp08}.

\begin{remark}[\cite{mp08}]
  Any weakly skew circuit of size $m$ can be written as a projection of $\Det_{m+1}$
\end{remark}


\section{The Hessian approach of Mignon-Ressayre}

 Let $A_{k,\ell}(X)$, $1\leq k,\ell\leq m$ be the affine linear functions over $\F[X]$ such that the following is true.
\begin{align*}
  f(X) = \det((A_{k,\ell}(X))_{1\leq k,\ell\leq m})
\end{align*}
 Consider a point $X_0\in \F^{n}$ such that $f(X_0)=0$. The affine linear functions $A_{k,\ell}(X)$ can be expressed as $L_{k,\ell}(X-X_0) + y_{k,\ell}$ where $L_{k,\ell}$ is a linear form and $y_{k,\ell}$ is a constant from the field. Thus, $(A_{k,\ell}(X))_{1\leq k,\ell\leq m} = (L_{k,\ell}(X-X_0))_{1\leq k,\ell\leq m} + Y_0$. If $f(X_0)=0$ then $\det(Y_0)=0$. Let $C$ and $D$ be two non-singular matrices such that $CY_0D$ is a diagonal matrix.

\begin{align*}
  CY_0D =
  \begin{pmatrix}
    0& 0\\
    0& I_s\\
  \end{pmatrix}
\end{align*}

Since $\det(Y_0)=0$, $s<m$. From the previous works \cite{von1987}, \cite{cai1990}, \cite{mr04}, and \cite{ccl2008}, it is enough to assume that $s=m-1$. Since the first row and the first column of $CY_0D$ are zero, we may multiply $CY_0D$ by $\diag(\det(C)^{-1},1,\dots,1)$ and $\diag(\det(D)^{-1},1,\dots,1)$ on the left and the right side. Without loss of generality, we may assume that $\det(C)=\det(D)=1$. By multiplying with $C$ and $D$ on the left and the right and suitably renaming $(L_{k,\ell}(X-X_0))_{1\leq k,\ell\leq m}$ and $Y_0$ we get
\begin{align*}
  f(X) = \det((L_{k,\ell}(X-X_0)_{1\leq k,\ell\leq m} + Y_0))
\end{align*}
where $Y_0 = \diag(0,1,\dots,1)$. 

We use $\hess_{f}(X)$ to denote the Hessian matrix of the iterated matrix multiplication and is defined as follows.
\begin{align*}
  \hess_{f}(X) &= (H_{s;ij,t;k\ell}(X))_{1\leq i,j\leq n, 1\leq s,t \leq d }\\
  H_{s;ij,t;k\ell}(X) &= \frac{\partial^2f(X)}{\partial x_{ij}^{(s)}\partial x_{k\ell}^{(t)}}
\end{align*}
where $x_{ij}^{(s)}$ and $x_{k\ell}^{(t)}$  denote the $(i,j)$th and $(k,\ell)$th entries of the variable sets $X^{(s)}$ and $X^{(t)}$ respectively. 

By taking second order derivatives and evaluating the Hessian matrices of $f(X)$ and $\det((A_{k,\ell}(X))_{1\leq k,\ell\leq m})$ at $X_0$, we obtain $\hess_{f}(X_0) = L\hess_{\det}(Y_0)L^{T}$ where $L$ is a $n\times m^2$ matrix with entries from the field. It follows that $\rank(\hess_{f}(X_0)) \leq \rank(\hess_{\det}(Y_0))$. It was observed in the earlier work  of \cite{mr04} and \cite{ccl2008} that it is relatively easy to get an upper bound for $\rank(\hess_{\det}(Y_0))$. 
The main task is to construct a point $X_0$ such that $f(X_0)=0$, yet the rank of $\hess_{f}(X_0)$ is high. 

\subsection{Upper bound for the rank of $\hess_{det}(Y_0)$}

 When we take a  partial derivative of the determinant polynomial with respect to the variable $x_{ij}$, what we get is the minor after striking out the row $i$ and column $j$. 
 The second order derivative of $\det(Y)$ with respect to the variables $y_{ij}$ and $y_{k\ell}$ eliminates the rows $\{i,k\}$ and the columns $\{j,\ell\}$. Considering the form of $Y_0$, the non-zero entries in $\hess_{\det}(Y_0)$ are obtained only if $1\in\{i,k\}$ and $1\in\{j,\ell\}$ and thus $(ij,k\ell)$ are of the form $(11,tt)$ or $(t1,1t)$ or $(1t,t1)$ for any $t>1$. Thus, $\rank(\hess_{\det}(Y_0)) = 2m$.
 
\subsection{Lower bounds}

\begin{theorem}[\cite{mr04}]
	Over a field of characteristic zero, for any $d>3$, rank of $\hess_{\Perm}(A)$ is $d^2$ where $A$ is a $d\times d$ sized matrix defined as follows.
	
	\begin{align*}
		(A)_{ij} = \begin{cases}
			d-1& \mbox{if $i=j=1$}\\
			1& \mbox{otherwise}
		\end{cases}
		% A =
% 	    \begin{bmatrix}
% 	      d-1& 1& \cdots& 1\\
% 	      1& 1& \cdots& 1\\
% 		  \vdots& \vdots& \vdots& \vdots\\
% 		  1& 1& \cdots& 1\\
% 	    \end{bmatrix}
	\end{align*}
\end{theorem}

\begin{theorem}[\cite{ccl2008}]
	Let $p > 2$ be a prime, then
	\begin{enumerate}
		\item If $p \neq 23$, then for any $n > 2$ satisfying $p\lvert 􏰄􏰄(n+1)$, $\Perm(M^n_1) \equiv 0 (\mod p)$ and $\rank(\hess_{\Perm}(M^n_1)) \geq (n-2)(n-3)$;
		\item If $p \neq 3, 5$, then for any $n > 1$ satisfying $p\lvert􏰄􏰄(n + 2)$, $\Perm(M^n_2) \equiv 0 (\mod p)$ and $\rank(\hess_{\Perm}(M^n_2)) \geq (n-2)(n-3)$;
	\end{enumerate}
	
	where $M^n_v$ is a $(n+1)\times(n+1)$ matrix over $\F^{(n+1)\times(n+1)}$ and $M^n_v = (M_{i,j}) : M_{(n+1),(n+1)} = v$; $M_{i,i} = M_{i,(n+1)} = M_{(n+1),i} = 1$ for all $i\in[n]$ and $M_{i,j}=0$ otherwise.
	
\end{theorem}


\section{Yabe's improvement~\cite{Yabe15}}

A recent result of Yabe~\cite{Yabe15} improves the lower bound to $\dc(\Perm_d) \geq (d-1)^2 + 1$ and we give an description of the proof now. 

The main idea in Yabe's proof is to use the determinantal complexity of a polynomial to a certain \emph{bi-linear rank} of a quadratic form closely related to the Hessian. 

\begin{definition}[Quadratic forms and associated matrices]
A quadratic form $Q$ over $n$-variables is a homogeneous degree $2$ polynomial $Q(\vecx) = \sum_{i,j} q_{ij} x_i x_j$. 

For every such quadratic form $Q$, we shall define the following symmetric matrix (that shall also be called $Q$) such that $Q_{i,j} = \frac{q_{ij}}{2}$ if $i\neq j$ (half the coefficient of $x_i x_j$) and $Q_{i,i} = q_{i,i}$ (the coefficient of $x_i^2$). 
\end{definition}

For example, the matrix corresponding to the quadratic form $x_1^2 + 2x_1x_2 + x_2^2$ would be the $2\times 2$ of ones.\\

For any point $X_0 \in \R^{d\times d}$, the Hessian of $\Perm_d$ at the point $X_0$ can be interpreted as a quadratic form as well. The following observation gives an alternate method to understand this quadratic form. 

\begin{observation}\label{obs:hessian-deg2-hom}
Let $f$ be any $n$-variate polynomial and let $\veca \in \R^n$Then the quadratic form associated with the Hessian of $f$ evaluated at $\veca$, denoted by $\hess(f)(\veca)$, can be equivalently expressed as
\[
\sum_{i,j} x_{i} x_{j} \cdot \pfrac{\partial^2 f}{\partial x_{i} \partial x_{j}}\inparen{\veca} \spaced{=} \Hom_2(f(\vecx + \veca))
\]
\end{observation}
\begin{proof}
The statement follows almost immediately when $f(\vecx + \veca)$ is expanded around $\mathbf{0}$ using the Taylor Expansion. 
\end{proof}


\noindent 
For any quadratic form $Q$, there is a very natural notation of rank that can be defined. 

\begin{definition}[Bi-linear rank of a quadratic form] For any quadratic form $Q$ we shall define its \emph{bi-linear rank}, denoted by $\brank(Q)$, as the smallest integer $r$ for which there exists linear forms $\ell_1,\ldots, \ell_r, \ell_1',\ldots,  \ell_r'$ such that 
\[
Q \spaced{=} \ell_1 \ell_1' + \cdots + \ell_r \ell_r'. \qedhere
\]
\end{definition}

\begin{observation}\label{obs:brank-rank}
A quadratic form $Q$ satisfies $\brank(Q) \leq r$ if and only if there exists a matrix $B$ such that $Q = B + B^T$ and $\rank(B) \leq r$. 
\end{observation}
\begin{proof}
TODO
\end{proof}

\subsection{Using $\brank$ to lower bound determinantal complexity}

The usual methods to get a bound on determinantal complexity is to find a suitable point $X_0 \in \R^{d\times d}$ such that $\Perm_d(X_0) = 0$ and look at the rank of the Hessian at $X_0$. Mignon and Ressayre~\cite{mr04} showed that $\rank(\hess(\Perm_d)(X_0))$ is upper-bounded by $2\cdot \dc(\Perm_d)$. Thus, one just needs to find a suitable matrix $X_0$ for which the Hessian has rank about $d^2$. 

Yabe~\cite{Yabe15} deviates from this plan and instead studies the $\brank$ of the Hessian. The first step in his approach is to connect the determinantal complexity to the $\brank$ of the Hessian. 


\begin{lemma}\label{lem:detcomplexity-to-brank} Let $f$ be an $n$-variate polynomial and $\veca \in \R^n$ be a point such that $f(\veca) = 0$. Then, $\brank(\Hom_2(f(\vecx + \veca))) \leq \max(\dc(f),4)$. 
\end{lemma}
\begin{proof}
Let $\dc(f) = m$. This means that $f$ can be computed as the determinant of an $m\times m$ matrix of linear polynomials in $\vecx$, that is there are matrices $A$ and $B$ such that  
\begin{eqnarray*}
f(\vecx + \veca) & = & \Det_m(A(\vecx) + B)
\end{eqnarray*}
Since $f(\veca) = 0$, we know that the matrix $B$ is not invertible. Hence, there exists some pair of matrices $S, T$ with $\det(S) = \det(T) = 1$ such that $S B T$ takes the form of a diagonal matrix with either ones or zeroes on the diagonal. Furthermore, since $\det(B) = 0$, we know that there must be at least one $0$ on the diagonal. Let $\Lambda_s$ refer to the diagonal matrix with $s$ zeroes on the diagonal followed by ones. Then, $S B T = \Lambda_s$ for some $s \geq 1$. Therefore 
\begin{eqnarray*}
f(\vecx + \veca) & = & \Det_m(S) \cdot \Det_m(A(\vecx) + B) \cdot \Det_m(T)\\
 & = & \Det_m( S (A(\vecx)) T + \Lambda_s)\\
 & = & \Det_m( A'(\vecx) + \Lambda_s).
\end{eqnarray*}
We are interested in the degree $2$ homogeneous component of the RHS. If $s > 2$, then every monomial in $\Det_m(A'(\vecx) + \Lambda_s)$ has degree at least $3$ so the homogeneous degree $2$ part is just zero. If $s = 2$, then the homogeneous degree $2$ part is just $A'_{11}(\vecx) A'_{22}(\vecx) - A'_{12}(\vecx) A'_{21}(\vecx)$ and hence $\brank(\Hom_2(f(\vecx + \veca)))\leq 4$. 

In the case of $s = 1$, it is easy to check that the only degree $2$ monomials are $A_{11}'(\vecx) A_{ii}(\vecx)$ and $A_{1i}'(\vecx) A_{i1}'(\vecx)$. Thus, any linear combination of them can be written as
\[
A_{11}'(\vecx) \cdot \ell  \;+\;  \sum_{i=2}^m \alpha_i \cdot A_{1i}'(\vecx)A_{i1}'(\vecx)
\]
and hence $\brank(\Hom_2(f(\vecx + \veca))) \leq m$
\end{proof}

In the original argument of Mignon and Ressayre, they get that $\rank(\hess(\Perm_d)(X_0)) \leq 2 \cdot \dc(\Perm_d)$ and this is the loss that eventually results in the lower bound of $d^2/2$. But \autoref{lem:detcomplexity-to-brank} saves this factor of two but we are now left with the task of showing that the quadratic form associated with $\hess(\Perm_d)(X_0)$ has large bi-linear rank. Yabe~\cite{Yabe15} does this by using \emph{Sylvester's Law of Inertia}.

\begin{lemma}[Sylvester's Law of Inertia]\label{lem:sylvester-law-of-inertia} Let $Q$ be a real symmetric matrix that has $n_+$ positive eigenvalues, $n_-$ negative eigenvalues. Then, if $Q = B + B^T$, we must have $\rank(B) \geq \max(n_+, n_-)$. 
\end{lemma}
\begin{proof}
TODO
\end{proof}

By \autoref{lem:sylvester-law-of-inertia} and \autoref{obs:brank-rank}, in order to lower-bound $\brank(\hess(\Perm_d)(X_0))$, we just need to show that $\hess(\Perm_d)(X_0)$ many eigenvalues of the same sign. This is the main technical calculation. 

\begin{lemma}\label{lem:X0-neg-eigenvalue-bound}
Let $X_0$ be the matrix defined by 
\[
X_0 \spaced{=} \insquare{\begin{array}{cccc}
1-d & 1 & \cdots & 1\\
1 & 1 & \cdots & 1\\
\vdots & \vdots & \ddots & \vdots\\
1 & 1 & \cdots & 1
\end{array}}
\]
that satisfies $\Perm_d(X_0) = 0$. The matrix $\hess(\Perm_d)(X_0)$ has $(d-1)^2 + 1$ negative eigenvalues. 
\end{lemma}

\noindent 
We shall defer this calculation to the end of the chapter and finish the lower bound. 

\begin{theorem}[\cite{Yabe15}]\label{thm:dc-comp-yabe}
$\dc(\Perm_d) \geq (d-1)^2 + 1$. 
\end{theorem}
\begin{proof}
Let $X_0$ be the matrix as in \autoref{lem:X0-neg-eigenvalue-bound}. By \autoref{lem:detcomplexity-to-brank}, we get that
\[
\brank(\Hom_2(\Perm_d(X + X_0))) \spaced{\leq} \dc(\Perm_d). 
\]
On the other hand, \autoref{obs:brank-rank}, \autoref{lem:sylvester-law-of-inertia} and  \autoref{lem:X0-neg-eigenvalue-bound} imply that 
\[
\brank(\Hom_2(\Perm_d(X + X_0))) \spaced{\geq} (d-1)^2 + 1
\]
forcing $\dc(\Perm_d) \geq (d-1)^2 + 1$. 
\end{proof}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
