\section{Introduction}\label{sec:introduction}
Polynomials originated in classical mathematical studies concerning
geometry and solutions to systems of equations. They feature in many
classical results in algebra, number theory and geometry - e.g. in
Galois and Abel's resolution of the solvability via radicals of a
quintic, Lagrange's theorem on expressing every natural number as a
sum of four squares and the impossibility of trisecting an angle
(using ruler and compass). In modern times, computer scientists began
to investigate as to what functions can be (efficiently)
computed. Polynomials being a natural class of functions, one is
naturally lead to the following question:
	\begin{center}
		\begin{minipage}{0.8\textwidth}
			{\em What is the optimum way to compute a given (family of) polynomial(s)?}
		\end{minipage}
	\end{center}
	Now the most natural way to compute a polynomial $f(x_1, x_2, \ldots, x_n) $ 
	over a field $\FF$ is to start with the input variables 
	$x_1, x_2, \ldots, x_n$ and then apply a sequence of basic 
	operations such as additions, subtractions and 
	multiplications\footnote{
		One can also allow more arithmetic operations such as division 
		and square roots. It turns out however that one can efficiently 
		simulate any circuit with divisions and square roots by another 
		circuit without these operations while incurring only an 
		polynomial factor increase in size.
	}
	in order to obtain the desired polynomial $f$. Such a computation 
	is called a straight line program. We often represent such a 
	straight-line program graphically as an arithmetic circuit - 
	wherein the overall computation corresponds to a directed acylic 
	graph whose source nodes are labelled with the input variables 
	$\{ x_1, x_2, \ldots, x_n \}$ and the internal nodes are labelled 
	with either $+$ or $\times$ (each internal node corresponds to one 
	computational step in the straight-line program). We typically allow 
	arbitrary constants from the underlying field on the incoming edges 
	of a $+$ gate so that a $+$ gate can in fact compute an arbitrary 
	$\FF$-linear combination of its inputs. The complexity of the 
	computation corresponds to the number of operations, also called 
	the size of the corresponding arithmetic circuit.
	With arithmetic circuits being the relevant model, the informal 
	question posed above can be formalized by defining the optimal 
	way to compute a given polyomial as the smallest arithmetic 
	circuit in terms of the size that computes it. While different aspects 
	of polynomials have been studied extensively in various areas of 
	mathematics, what is unique to computer science is the endeavour to 
	prove upper and lower bounds on the size of arithmetic circuits 
	computing a given (family of) polynomials. Here we give a biased 
	survey of this area, focusing mostly on lower bounds. Note that
	there are already two excellent surveys of this area - one by Avi 
	Wigderson \cite{aviSurvey} and the other by Amir Shpilka and Amir 
	Yehudayoff \cite{sy}\footnote{
		A more specialized survey by Chen, Kayal and Wigderson \cite{ckw11}
		focuses on the applications of partial derivatives in understanding 
		the structure and complexity of polynomials. 
	}. Our intention in writing the survey is the underlying hope 
	that revisiting and assimilating the known results pertaining to 
	circuit lower bounds will in turn help us make progress on this 
	beautiful problem. Consequently we mostly present here those 
	results which we for some reason felt we did not understand 
	comprehensively enough. We conclude with some recent lower 
	bound results for homogeneous bounded depth formulas. Some 
	notable lower bound results that we are unable to present 
	here due to space and time constraints are as follows. A 
	quadratic lower bound for depth three circuits by Shpilka and Wigderson \cite{sw2001}, for bounded occur 
	bounded depth formulas by Agrawal, Saha, Saptharishi and 
	Saxena \cite{ASSS12} and the $n^{1 + \Omega(1/r)}$ lower bound for 
	circuits of depth $r$ by Raz \cite{raz10}.\\

\noindent {\bf Overview.}
The state of affairs in arithmetic complexity is such that despite a
lot of attention we still have only modest lower bounds for general
circuits and formulas. In order to make progress, recent work has
focused on restricted subclasses.  We first present the best known
lower bound for general circuits due to Baur and Strassen \cite{BS83},
and a lower bound for formulas due to Kalorkoti
\cite{k85}. The subsequent lower bounds that we present
follow a common roadmap and we articulate this in Section
\ref{sec:roadmap}, and present some simple lower bounds to help the
reader gain familiarity. We then present (a slight generalization of)
an exponential lower bound for monotone circuits due to Jerrum and
Snir \cite{js82}.  Moving on to more restricted (but still nontrivial
and interesting) models, we first present an exponential lower bound
for depth three circuits over finite fields due to Grigoriev and
Karpinski \cite{grigoriev98} and multilinear formulas. We conclude
with some recent progress on lower bounds for homogeneous depth four
circuits.
	 
\begin{remark*} Throughout the article, we shall use $\Det_n$ and $\Perm_n$ to refer to the determinant and permanent respectively of a symbolic $n\times n$ matrix $\inparen{\!\inparen{x_{ij}}\!}_{1\leq i,j\leq n}$. 

\end{remark*}

	
	
	
	

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "lowerbounds"
%%% End: 
